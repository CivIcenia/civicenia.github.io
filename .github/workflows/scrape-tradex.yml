name: Scrape Tradex dataset

on:
  schedule:
    - cron: '*/30 * * * *' # every 30 minutes; change to '0 */1 * * *' for hourly
  workflow_dispatch: {}

permissions:
  contents: write

concurrency:
  group: scrape-tradex
  cancel-in-progress: true

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          persist-credentials: true

      - name: Fetch Tradex data
        run: |
          set -euo pipefail
          mkdir -p public

          # Fetch the dataset using Node.js with proxy fallback
          node - <<'NODE'
          const fs = require('fs');
          const https = require('https');
          const http = require('http');

          const TARGET_URL = 'https://api.tradex.civinfo.net/exchanges/search';
          
          // List of proxies to try in order
          const PROXIES = [
              { name: 'Direct API', url: TARGET_URL },
              { name: 'AllOrigins (Raw)', url: 'https://api.allorigins.win/raw?url=' + encodeURIComponent(TARGET_URL) },
              { name: 'YaCDN', url: 'https://yacdn.org/proxy/' + TARGET_URL },
              { name: 'HTMLDriven', url: 'https://cors-anywhere.htmldriven.com/?url=' + TARGET_URL },
              { name: 'CodeTabs', url: 'https://api.codetabs.com/v1/proxy?quest=' + encodeURIComponent(TARGET_URL) },
              { name: 'CORSProxy.io', url: 'https://corsproxy.io/?' + encodeURIComponent(TARGET_URL) },
              { name: 'ThingProxy', url: 'https://thingproxy.freeboard.io/fetch/' + TARGET_URL },
              { name: 'AllOrigins (JSON)', url: 'https://api.allorigins.win/get?url=' + encodeURIComponent(TARGET_URL) },
              { name: 'CORS Bridged', url: 'https://cors.bridged.cc/' + TARGET_URL }
          ];

          const searchParams = {}; // Empty search to get all exchanges

          async function tryFetch(proxy, attemptIndex) {
              console.log(`Attempting search via ${proxy.name}...`);
              const isAllOriginsJson = proxy.url.includes('allorigins.win/get');
              
              const timeoutMs = attemptIndex < 2 ? 5000 : 8000;
              
              return new Promise((resolve, reject) => {
                  const url = new URL(proxy.url);
                  const lib = url.protocol === 'https:' ? https : http;
                  const body = JSON.stringify(searchParams);
                  
                  const options = {
                      method: 'POST',
                      headers: {
                          'Content-Type': 'application/json',
                          'Content-Length': Buffer.byteLength(body)
                      },
                      timeout: timeoutMs
                  };
                  
                  const req = lib.request(url, options, (res) => {
                      if (res.statusCode < 200 || res.statusCode >= 300) {
                          return reject(new Error(`${proxy.name} returned status ${res.statusCode}`));
                      }
                      
                      let data = '';
                      res.on('data', (chunk) => data += chunk);
                      res.on('end', () => {
                          try {
                              let json = JSON.parse(data);
                              // AllOrigins /get wraps the response in a JSON object with a 'contents' field
                              if (isAllOriginsJson && json.contents) {
                                  json = JSON.parse(json.contents);
                              }
                              
                              if (json && (json.exchanges || Array.isArray(json))) {
                                  resolve(json);
                              } else {
                                  reject(new Error('Invalid JSON structure'));
                              }
                          } catch (e) {
                              reject(new Error(`${proxy.name} response was not valid JSON: ${e.message}`));
                          }
                      });
                  });
                  
                  req.on('error', reject);
                  req.on('timeout', () => {
                      req.destroy();
                      reject(new Error(`${proxy.name} request timed out after ${timeoutMs}ms`));
                  });
                  
                  req.write(body);
                  req.end();
              });
          }

          async function fetchWithFallback() {
              for (let i = 0; i < PROXIES.length; i++) {
                  try {
                      const result = await tryFetch(PROXIES[i], i);
                      console.log(`✓ Successfully fetched data via ${PROXIES[i].name}`);
                      return result;
                  } catch (err) {
                      console.error(`✗ ${PROXIES[i].name} failed: ${err.message}`);
                      if (i === PROXIES.length - 1) {
                          throw new Error('All proxy attempts failed');
                      }
                  }
              }
          }

          (async () => {
              try {
                  const data = await fetchWithFallback();
                  
                  // Normalize output and add timestamp
                  const out = Array.isArray(data)
                      ? { _scraped_at: new Date().toISOString(), data }
                      : (typeof data === 'object' && data !== null
                          ? Object.assign({}, data, { _scraped_at: new Date().toISOString() })
                          : { _scraped_at: new Date().toISOString(), data });
                  
                  fs.writeFileSync('public/data.json', JSON.stringify(out, null, 2));
                  console.log('Successfully saved data to public/data.json');
              } catch (err) {
                  console.error('Failed to fetch Tradex data:', err.message);
                  process.exit(1);
              }
          })();
          NODE

      - name: Publish scraped data to scraped-data branch
        env:
          REPO_URL: https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
        run: |
          set -euo pipefail
          git config --global user.name "github-actions[bot]"
          git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Prepare a minimal temporary repository to push only the scraped artefacts to a dedicated branch.
          TMPDIR="$(mktemp -d)"
          echo "Using tempdir: $TMPDIR"
          # Copy files if they exist
          mkdir -p "$TMPDIR"
          cp public/data.json "$TMPDIR/" 2>/dev/null || true
          # scraper-status.json may have been created earlier; ensure it exists
          echo "{\"last_success\":\"$(date -u +"%Y-%m-%dT%H:%M:%SZ")\"}" > public/scraper-status.json
          cp public/scraper-status.json "$TMPDIR/" 2>/dev/null || true

          cd "$TMPDIR"
          git init
          git remote add origin "${REPO_URL}"
          git add -A
          git commit -m "chore(scraper): update scraped data [skip ci]" || true
          # Force-push to the dedicated branch so updates replace previous snapshots.
          git push -f origin HEAD:refs/heads/scraped-data
          # Cleanup
          cd /
          rm -rf "$TMPDIR"

      - name: Success message
        run: echo "Scrape job completed."